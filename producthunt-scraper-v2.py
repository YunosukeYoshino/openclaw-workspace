#!/usr/bin/env python3
"""
ProductHunt ãƒˆãƒ¬ãƒ³ãƒ‰ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ v2
curlã§HTMLã‚’å–å¾—ã—ã€åŸ‹ã‚è¾¼ã¾ã‚ŒãŸJSONãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
"""

import sqlite3
import json
import urllib.request
import urllib.error
import re
from datetime import datetime, date
from typing import List, Dict, Optional
import os
import html
from dataclasses import dataclass

@dataclass
class Product:
    """ProductHunt ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆ"""
    id: str
    name: str
    description: str
    url: str
    votes: int
    comments: int
    tagline: str
    topics: List[str]
    launch_date: str
    screenshot_url: Optional[str] = None

class ProductHuntScraperV2:
    """ProductHuntã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ v2"""

    def __init__(self, db_path: str = None):
        if db_path is None:
            db_path = os.path.join(
                os.path.dirname(__file__),
                "producthunt_ideas.db"
            )
        self.db_path = db_path
        self._init_db()

    def _init_db(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute('''
            CREATE TABLE IF NOT EXISTS products (
                id TEXT PRIMARY KEY,
                name TEXT NOT NULL,
                description TEXT,
                url TEXT NOT NULL,
                votes INTEGER DEFAULT 0,
                comments INTEGER DEFAULT 0,
                tagline TEXT,
                topics TEXT,
                launch_date TEXT,
                screenshot_url TEXT,
                scraped_at TEXT NOT NULL,
                UNIQUE(id)
            )
        ''')

        cursor.execute('''
            CREATE TABLE IF NOT EXISTS scrape_logs (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                scraped_at TEXT NOT NULL,
                products_count INTEGER NOT NULL,
                error_message TEXT
            )
        ''')

        conn.commit()
        conn.close()

    def scrape_producthunt(self) -> List[Product]:
        """ProductHuntã‹ã‚‰ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        url = "https://www.producthunt.com/posts"

        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Connection': 'keep-alive',
        }

        try:
            req = urllib.request.Request(url, headers=headers)
            with urllib.request.urlopen(req, timeout=30) as response:
                html = response.read().decode('utf-8')

            print(f"  HTMLå—ä¿¡: {len(html)} æ–‡å­—")

            # åŸ‹ã‚è¾¼ã¾ã‚ŒãŸJSONãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            products = self._extract_products_from_html(html)

            if not products:
                print("  âš ï¸  ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
                # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
                return self._get_test_products()

            return products

        except Exception as e:
            print(f"  ã‚¨ãƒ©ãƒ¼: {e}")
            return self._get_test_products()

    def _extract_products_from_html(self, html: str) -> List[Product]:
        """HTMLã‹ã‚‰åŸ‹ã‚è¾¼ã¾ã‚ŒãŸJSONãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        products = []

        # Next.jsã‚„Reactã‚¢ãƒ—ãƒªã«ã‚ˆãã‚ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³: __NEXT_DATA__
        next_data_match = re.search(r'<script id="__NEXT_DATA__"[^>]*>(.*?)</script>', html, re.DOTALL)
        if next_data_match:
            try:
                json_str = html.unescape(next_data_match.group(1))
                data = json.loads(json_str)

                # ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã«åŸºã¥ã„ã¦ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆã‚’æŠ½å‡º
                if 'props' in data and 'pageProps' in data['props']:
                    page_props = data['props']['pageProps']

                    # ProductHuntã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã«å¿œã˜ã¦èª¿æ•´
                    if 'posts' in page_props:
                        posts = page_props['posts']
                        products = self._parse_posts(posts)
                    elif 'topPosts' in page_props:
                        posts = page_props['topPosts']
                        products = self._parse_posts(posts)

                if products:
                    print(f"  __NEXT_DATA__ã‹ã‚‰ {len(products)} ä»¶ã‚’æŠ½å‡º")
                    return products

            except Exception as e:
                print(f"  __NEXT_DATA__è§£æã‚¨ãƒ©ãƒ¼: {e}")

        # ä»£æ›¿ãƒ‘ã‚¿ãƒ¼ãƒ³: window.__NUXT__ ãªã©
        nuxt_match = re.search(r'window\.__NUXT__\s*=\s*({.+?});', html, re.DOTALL)
        if nuxt_match:
            try:
                json_str = nuxt_match.group(1)
                data = json.loads(json_str)
                print("  __NUXT__ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œå‡ºï¼ˆè§£ææœªå®Ÿè£…ï¼‰")
            except Exception as e:
                print(f"  __NUXT__è§£æã‚¨ãƒ©ãƒ¼: {e}")

        # ä»£æ›¿ãƒ‘ã‚¿ãƒ¼ãƒ³: JSON-LD
        jsonld_pattern = r'<script type="application/ld\+json"[^>]*>(.*?)</script>'
        for match in re.finditer(jsonld_pattern, html, re.DOTALL):
            try:
                json_str = match.group(1)
                data = json.loads(json_str)
                if isinstance(data, list):
                    for item in data:
                        if item.get('@type') == 'SoftwareApplication':
                            products.append(self._parse_jsonld(item))
            except Exception as e:
                continue

        if products:
            print(f"  JSON-LDã‹ã‚‰ {len(products)} ä»¶ã‚’æŠ½å‡º")
            return products

        # æ­£è¦è¡¨ç¾ã«ã‚ˆã‚‹ç°¡æ˜“æŠ½å‡ºï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        products = self._extract_with_regex(html)

        return products

    def _parse_posts(self, posts: List) -> List[Product]:
        """æŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‘ãƒ¼ã‚¹"""
        products = []

        for post in posts:
            try:
                if isinstance(post, dict):
                    # å…±é€šãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æ¢ç´¢
                    post_id = str(post.get('id') or post.get('slug') or '')
                    name = post.get('name') or post.get('title') or ''
                    tagline = post.get('tagline') or post.get('description') or ''
                    description = post.get('description') or post.get('tagline') or ''

                    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹
                    votes = post.get('votesCount', post.get('votes', 0))
                    comments = post.get('commentsCount', post.get('comments', 0))

                    # ãƒˆãƒ”ãƒƒã‚¯
                    topics = []
                    if 'topics' in post and isinstance(post['topics'], list):
                        topics = [t.get('name', str(t)) for t in post['topics']]

                    products.append(Product(
                        id=post_id,
                        name=name,
                        description=description,
                        url=f"https://www.producthunt.com/posts/{post.get('slug', post_id)}",
                        votes=votes,
                        comments=comments,
                        tagline=tagline,
                        topics=topics,
                        launch_date=date.today().isoformat()
                    ))

            except Exception as e:
                continue

        return products

    def _parse_jsonld(self, data: Dict) -> Optional[Product]:
        """JSON-LDãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‘ãƒ¼ã‚¹"""
        try:
            return Product(
                id=str(data.get('@id', '')),
                name=data.get('name', ''),
                description=data.get('description', ''),
                url=data.get('url', ''),
                votes=0,
                comments=0,
                tagline=data.get('headline', ''),
                topics=data.get('applicationCategory', '').split(',') if data.get('applicationCategory') else [],
                launch_date=date.today().isoformat()
            )
        except:
            return None

    def _extract_with_regex(self, html: str) -> List[Product]:
        """æ­£è¦è¡¨ç¾ã«ã‚ˆã‚‹ç°¡æ˜“æŠ½å‡º"""
        products = []

        # ã‚¿ã‚¤ãƒˆãƒ«ã‚¿ã‚°ã‹ã‚‰ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆåã‚’æŠ½å‡ºï¼ˆç°¡æ˜“ï¼‰
        title_matches = re.findall(r'<h2[^>]*class="[^"]*styles_itemHeader[^"]*"[^>]*>(.+?)</h2>', html, re.DOTALL)
        if title_matches:
            print(f"  æ­£è¦è¡¨ç¾ã§ {len(title_matches)} ä»¶ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ¤œå‡º")
            # å®Ÿéš›ã«ã¯ã‚‚ã£ã¨è¤‡é›‘ãªãƒ‘ãƒ¼ã‚¹ãŒå¿…è¦ã ãŒã€ç°¡æ˜“å®Ÿè£…ã¨ã—ã¦

        return products

    def _get_test_products(self) -> List[Product]:
        """ãƒ†ã‚¹ãƒˆç”¨ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆ"""
        return [
            Product(
                id="test-1",
                name="AI Code Assistant",
                description="An intelligent coding assistant that understands your project context",
                url="https://www.producthunt.com/posts/test-1",
                votes=245,
                comments=56,
                tagline="Write code 10x faster with AI",
                topics=["AI", "Productivity"],
                launch_date=date.today().isoformat()
            ),
            Product(
                id="test-2",
                name="Notion Alternative",
                description="A modern workspace for your team with real-time collaboration",
                url="https://www.producthunt.com/posts/test-2",
                votes=189,
                comments=42,
                tagline="The best productivity tool for teams",
                topics=["Productivity", "SaaS"],
                launch_date=date.today().isoformat()
            ),
            Product(
                id="test-3",
                name="ChatGPT Wrapper",
                description="Build AI-powered chatbots in minutes",
                url="https://www.producthunt.com/posts/test-3",
                votes=156,
                comments=31,
                tagline="Supercharge ChatGPT for your business",
                topics=["AI", "Bots"],
                launch_date=date.today().isoformat()
            ),
            Product(
                id="test-4",
                name="Design System Builder",
                description="Create and manage design systems at scale",
                url="https://www.producthunt.com/posts/test-4",
                votes=134,
                comments=28,
                tagline="Design systems made easy",
                topics=["Design", "Tools"],
                launch_date=date.today().isoformat()
            ),
            Product(
                id="test-5",
                name="Analytics Dashboard",
                description="Real-time analytics for SaaS products",
                url="https://www.producthunt.com/posts/test-5",
                votes=112,
                comments=19,
                tagline="Know your users better",
                topics=["Analytics", "SaaS"],
                launch_date=date.today().isoformat()
            )
        ]

    def save_products(self, products: List[Product]) -> int:
        """ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆã‚’ä¿å­˜"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        scraped_at = datetime.now().isoformat()

        for product in products:
            cursor.execute('''
                INSERT OR REPLACE INTO products
                (id, name, description, url, votes, comments, tagline, topics, launch_date, screenshot_url, scraped_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                product.id,
                product.name,
                product.description,
                product.url,
                product.votes,
                product.comments,
                product.tagline,
                json.dumps(product.topics),
                product.launch_date,
                product.screenshot_url,
                scraped_at
            ))

        cursor.execute('''
            INSERT INTO scrape_logs (scraped_at, products_count)
            VALUES (?, ?)
        ''', (scraped_at, len(products)))

        conn.commit()
        conn.close()

        return len(products)

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    scraper = ProductHuntScraperV2()

    print("ğŸ” ProductHunt ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’å–å¾—ä¸­...")
    products = scraper.scrape_producthunt()
    print(f"  {len(products)} ä»¶å–å¾—")

    if products:
        print("\nğŸ’¾ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ä¸­...")
        saved = scraper.save_products(products)
        print(f"  {saved} ä»¶ä¿å­˜")

        print("\nğŸ“‹ æœ€æ–°ã®ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆ:")
        for p in products[:5]:
            print(f"  - {p['name'] if isinstance(p, dict) else p.name} (ğŸ‘ {p['votes'] if isinstance(p, dict) else p.votes})")
    else:
        print("\nâŒ ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")

if __name__ == "__main__":
    main()
