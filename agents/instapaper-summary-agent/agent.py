#!/usr/bin/env python3
"""
Instapaper Summary Agent
Instapaper RSSã‹ã‚‰è¨˜äº‹ã‚’å–å¾—ã—ã¦è¦ç´„ã—ã€Discordã«é€šçŸ¥ã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
"""

import os
import json
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import feedparser
import requests
from bs4 import BeautifulSoup

from db import Database
from discord import DiscordClient

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class ArticleSummarizer:
    """è¨˜äº‹ã®è¦ç´„ã‚’è¡Œã†ã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        pass

    def fetch_article_content(self, url: str) -> Optional[str]:
        """è¨˜äº‹ã®æœ¬æ–‡ã‚’å–å¾—ã™ã‚‹"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, 'html.parser')

            # ä¸è¦ãªè¦ç´ ã‚’å‰Šé™¤
            for tag in soup(['script', 'style', 'nav', 'footer', 'aside', 'header']):
                tag.decompose()

            # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡ºï¼ˆã‚·ãƒ³ãƒ—ãƒ«ãªå®Ÿè£…ï¼‰
            content = ''
            for tag in soup.find_all(['p', 'h1', 'h2', 'h3']):
                text = tag.get_text(strip=True)
                if text and len(text) > 50:
                    content += text + '\n'

            return content[:3000]  # 3000æ–‡å­—ã«åˆ¶é™

        except Exception as e:
            logger.error(f"Failed to fetch article content from {url}: {e}")
            return None

    def summarize_article(self, title: str, url: str, content: str) -> str:
        """è¨˜äº‹ã‚’è¦ç´„ã™ã‚‹"""
        if not content:
            return f"{title}\n\nè¦ç´„ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ"

        # ç°¡æ˜“çš„ãªè¦ç´„ï¼ˆæœ€åˆã®æ•°æ®µè½ã‚’æŠ½å‡ºï¼‰
        lines = content.split('\n')
        summary_lines = []
        char_count = 0

        for line in lines:
            if char_count + len(line) > 800:
                break
            if line.strip():
                summary_lines.append(line)
                char_count += len(line)

        summary = '\n'.join(summary_lines)
        return f"{title}\n\n{summary}\n...\n\nå…ƒè¨˜äº‹: {url}"


class InstapaperAgent:
    """Instapaper RSSã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""

    def __init__(self):
        self.rss_url = os.getenv('INSTAPAPER_RSS_URL', '')
        self.discord_webhook_url = os.getenv('DISCORD_WEBHOOK_URL', '')
        self.db = Database()
        self.discord = DiscordClient(self.discord_webhook_url)
        self.summarizer = ArticleSummarizer()

        if not self.rss_url:
            raise ValueError("INSTAPAPER_RSS_URL environment variable is required")

    def fetch_rss_items(self) -> List[Dict]:
        """RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰ã‚¢ã‚¤ãƒ†ãƒ ã‚’å–å¾—"""
        try:
            feed = feedparser.parse(self.rss_url)
            items = []

            for entry in feed.entries:
                item = {
                    'title': entry.get('title', ''),
                    'url': entry.get('link', entry.get('guid', '')),
                    'description': entry.get('description', ''),
                    'pub_date': entry.get('published', '')
                }
                items.append(item)

            logger.info(f"Fetched {len(items)} items from RSS")
            return items

        except Exception as e:
            logger.error(f"Failed to fetch RSS feed: {e}")
            return []

    def is_duplicate(self, url: str) -> bool:
        """URLãŒé‡è¤‡ã—ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        return self.db.url_exists(url)

    def process_article(self, article: Dict) -> Optional[str]:
        """è¨˜äº‹ã‚’å‡¦ç†ã—ã¦è¦ç´„ã‚’ç”Ÿæˆ"""
        url = article['url']

        # é‡è¤‡ãƒã‚§ãƒƒã‚¯
        if self.is_duplicate(url):
            logger.info(f"Skipping duplicate article: {url}")
            return None

        # è¨˜äº‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—
        content = self.summarizer.fetch_article_content(url)

        # è¦ç´„ã‚’ç”Ÿæˆ
        summary = self.summarizer.summarize_article(
            article['title'],
            url,
            content or article['description']
        )

        # URLã‚’ä¿å­˜ï¼ˆé‡è¤‡é˜²æ­¢ç”¨ï¼‰
        self.db.save_url(url, article['title'])

        return summary

    def send_discord_notification(self, summary: str, index: int, total: int):
        """Discordã«é€šçŸ¥ã‚’é€ä¿¡"""
        if not self.discord_webhook_url:
            logger.warning("Discord webhook URL not configured")
            return

        header = f"ğŸ“° Instapaper è¨˜äº‹é€šçŸ¥ ({index}/{total})"
        footer = f"\nğŸ• {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"

        message = f"{header}\n\n{summary}{footer}"

        self.discord.send_message(message)

    def run(self):
        """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè¡Œ"""
        logger.info("Starting Instapaper Summary Agent")

        # RSSã‹ã‚‰ã‚¢ã‚¤ãƒ†ãƒ ã‚’å–å¾—
        items = self.fetch_rss_items()

        if not items:
            logger.info("No articles found in RSS feed")
            return

        # å¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆ30æ—¥ä»¥ä¸Šå‰ï¼‰
        self.db.cleanup_old_entries(days=30)

        # è¨˜äº‹ã‚’å‡¦ç†
        new_articles = 0
        for i, article in enumerate(items, 1):
            summary = self.process_article(article)
            if summary:
                new_articles += 1
                self.send_discord_notification(summary, i, len(items))

        logger.info(f"Processed {new_articles} new articles")


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    try:
        agent = InstapaperAgent()
        agent.run()
    except Exception as e:
        logger.error(f"Agent failed: {e}")
        raise


if __name__ == '__main__':
    main()
